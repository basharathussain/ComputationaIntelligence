{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"_final-demo-06thDec.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"6Dl0WJsQjLh-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOkGSr1FjahT","colab_type":"code","outputId":"60426666-1a47-4248-cdb0-51c05fe9ade0","executionInfo":{"status":"ok","timestamp":1575574795259,"user_tz":-300,"elapsed":2372,"user":{"displayName":"Basharat Hussain","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDXtH5g0C7HMyOERBmIREHU1ZvtrNx98-DPcRb_=s64","userId":"13397079541735527718"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D3HvE62TjUHK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":363},"outputId":"9fc95592-df2a-4f43-e6fa-c2a4829102a7","executionInfo":{"status":"error","timestamp":1575574883255,"user_tz":-300,"elapsed":12106,"user":{"displayName":"Basharat Hussain","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDXtH5g0C7HMyOERBmIREHU1ZvtrNx98-DPcRb_=s64","userId":"13397079541735527718"}}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Nov 24 15:08:06 2019\n","\n","@author: Bash\n","\"\"\"\n","\"\"\"\n","###########################\n","Problem #8 on google colab \n","###########################\n","\"\"\"\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv), data manipulation as in SQL\n","import matplotlib.pyplot as plt\n","from datetime import datetime, date, time, timedelta\n","import matplotlib.dates as mdates\n","from sklearn.metrics import mean_squared_error, mean_absolute_error \n","from math import sqrt\n","from pandas import DataFrame\n","from pandas import concat\n","from sklearn.preprocessing import MinMaxScaler\n","from numpy import array\n","import time \n","from pandas import read_csv\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers.recurrent import LSTM\n","from keras.layers import Bidirectional\n","from keras.layers import Flatten\n","from keras.layers import TimeDistributed\n","from keras.layers.convolutional import Conv1D\n","from keras.layers.convolutional import MaxPooling1D\n","from keras.layers import ConvLSTM2D\n","from keras.layers.recurrent import GRU\n","from keras.optimizers import SGD\n","from keras.optimizers import Adam, Adagrad, RMSprop \n","from keras.layers.recurrent import SimpleRNN\n","\n","\n","\n","from statsmodels.tsa.arima_model import ARIMA\n","\n","from IPython.display import display # to display images\n","import texttable as tt\n","\n","from sys import exit\n","\n","\n","from imageUtil import imgUtil\n","from models import SupervisedDBNRegression\n","\n","### SECTION 11\n","RUN_ON_COLAB = 1      #0 = local, 1 = colab\n","RUN_ON_DATASET = 1  # 0=100-series, 1 = PeMS, 2=Shampoo\n","EPOCHS = [20, 50, 100, 200, 300, 400, 500, 1000] \n","#EPOCHS = EPOCHS[:2]\n","REPEATS = 2 #30     # number of times you like to repeat EPOCHs Experienet \n","\n","ALGOS =[[1001, 'Vanilla LSTM'], \n","        [1002, 'Stacked LSTM'], \n","        [1003, 'Bidirectional LSTM'],\n","        [1006, 'GRU'],\n","        [1007, 'ARIMA'], \n","        [1008, 'Vanilla LSTM w. HyperParams'], \n","        [1009, 'LSTM from scratch w/o library'],\n","        [1010, 'SAE'], \n","        [1011, 'DBN'],\n","        [1012, 'FFNN'], \n","        [1013, 'MLP'], \n","        [1014, 'RNN'], \n","        [1015, 'SVM'],\n","        [1016, 'KNN'], \n","        [1017, 'T-LSTM'], \n","        [1018, 'T-GRU'],\n","        [2001, 'CNN LSTM'], \n","        [2002, 'ConvLSTM'], ]\n","\n","ALGOKEYS = [x[0] for x in ALGOS]\n","ALGOKEYS = [1001, 1002, 1003, 1006, 1007, 1011, 1014, 2001]\n","#ALGOKEYS = [1001, 1002, 1007, 1011,  2001] \n","\n","PMES_FILE_START_INDEX = 1\n","PMES_FILE_END_INDEX = 15\n","FLOW_LEVEL = 2    # 0 = hour level, 1 = minute level, 2 = both levels\n","FREEWAY_NO = '5'  #Station No 5 to filter data with\n","#STATION_NO = '99'  #Station No 99 to filter data with\n","\n","SEED_DATE = datetime(2019, 1, 1)\n","dt0 = SEED_DATE \n","FILE_INDICES = range(PMES_FILE_START_INDEX, PMES_FILE_END_INDEX)  # traiing from 1-27 days\n","Validation_dataset_size_percentage = 15 \n","Test_dataset_size_percentage = 15 \n","arr_Epochs = EPOCHS \n","\n","global_arr_Plots= []\n","\n","\n","def mean_absolute_percentage_error(y_true, y_pred): \n","    y_true, y_pred = np.array(y_true), np.array(y_pred)\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","# Split data into train, validation and test sets\n","def split_data(raw_data):\n","    data = []\n","    \n","    raw_data = np.asmatrix(raw_data) \n","    data = np.array(raw_data);\n","    \n","    valid_set_size = int(np.round(Validation_dataset_size_percentage/100*data.shape[0]));  \n","    test_set_size = int(np.round(Test_dataset_size_percentage/100*data.shape[0]));\n","    train_set_size = data.shape[0] - (valid_set_size + test_set_size);\n","    \n","     #Separating the datasets\n","    X_train = data[:train_set_size,:-1]\n","    y_train = data[:train_set_size,-1]\n","    \n","    X_valid = data[train_set_size:train_set_size+valid_set_size,:-1]\n","    y_valid = data[train_set_size:train_set_size+valid_set_size,-1]\n","    \n","    X_test = data[train_set_size+valid_set_size:,:-1]\n","    y_test = data[train_set_size+valid_set_size:,-1]\n","    \n","    return [X_train, y_train, X_valid, y_valid, X_test, y_test]\n","    \n","    \n","    \n","def ReadPeMSFiles(fileIndices):\n","        \n","    datasetFolder = \"I:/10. PeMS dataset/5-minStation-dist10\" if RUN_ON_COLAB == 0 else \"/content/drive/My Drive/Google-CoLab/Problem#8 - My Paper LSTM GRU on PeMS data/Dataset-Jan2019\"\n","    \n","    file_data = []   \n","    for fileNo in fileIndices:\n","        if(fileNo < 10):\n","            file = open(datasetFolder + \"/d10_text_station_5min_2019_01_0\"+str(fileNo)+\".txt\",\"r\")\n","        else:\n","            file = open(datasetFolder + \"/d10_text_station_5min_2019_01_\"+str(fileNo)+\".txt\",\"r\")\n","            \n","        print (fileNo)\n","        \n","        previous_timestamp = -1\n","        #Repeat for each song in the text file\n","        for line in file:\n","          \n","          #Let's split the line into an array called \"fields\" using the \";\" as a separator:\n","          fields = line.split(\",\")\n","              \n","          #and let's extract the data:\n","          #to_see_if_needed = fields[0]\n","          if(fields[3] == FREEWAY_NO):  #Station No 5 only\n","              \n","              dt = datetime.strptime(fields[0], \"%m/%d/%Y %H:%M:%S\")\n","              delta = dt - dt0\n","              days = delta.days\n","              total_seconds = days*24*60*60 + delta.seconds\n","              # to remove the whitespaces and one quote '\n","              speed = fields[9].strip(\"\\r\\n\\t '\")\n","              if speed == '':\n","                  speed =  '0'\n","              speed = float(speed)\n","              \n","              if(previous_timestamp == total_seconds): \n","                  location = file_data[-1]\n","                  # if program manages to get\n","                  file_data[-1] = [location[0], fields[0], (speed + location[2] )/2.0] \n","              else:              \n","                  # if it doesn't find 5 this\n","                  file_data.append([total_seconds, fields[0], speed]) \n","                  previous_timestamp = total_seconds\n","        \n","        #It is good practice to close the file at the end to free up resources   \n","        file.close()\n"," \n","    return file_data\n","\n","#print(file_data[0:10]) \n","def Date_parser(x):\n","\treturn datetime.strptime('190'+x, '%Y-%m')\n"," \n","#############################################\n","### Step: read dataset files PeMS\n","#############################################\n","\n","if(RUN_ON_DATASET == 1):  \n","    df = ReadPeMSFiles(FILE_INDICES)\n","    ds = pd.Series([i[2] for i in df], index=[i[1] for i in df])\n","elif (RUN_ON_DATASET == 2):\n","   ds = read_csv('shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=Date_parser)\n","   df = [[x, ds[x]] for x in ds.index]\n","\n","if (RUN_ON_DATASET != 0):\n","    #Creating a copy of the data to pass through the parser\n","    raw_set = df.copy()\n","    \n","     #Sep out the data\n","    X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(raw_set);\n","    \n","    y_train = np.array(list(map(float,y_train.tolist())))\n","    y_valid = np.array(list(map(float,y_valid.tolist())))\n","    y_test = np.array(list(map(float,y_test.tolist())))\n","    \n","    #Checking the shape of each of the data \n","    print('x_train.shape = ',X_train.shape)\n","    print('y_train.shape = ', y_train.shape)\n","    print('x_valid.shape = ',X_valid.shape)\n","    print('y_valid.shape = ', y_valid.shape)\n","    print('x_test.shape = ', X_test.shape)\n","    print('y_test.shape = ',y_test.shape)\n","\n","#Plot a graph for each of train, valid and test sets\n","print(\"---------------PLOT a minute level graph first --------------------\") \n","if RUN_ON_DATASET == 1:\n","    x_axis = X_train[0:,1].tolist()\n","    x_axis = [datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\") for x in x_axis]\n","    y_axis = y_train   \n","    lines=plt.plot(x_axis, y_axis, label=\"Per-minute real flow - train\")\n","    plt.setp(lines, color='b', linewidth=1.2, ls='-')\n","       \n","    x_axis = X_valid[0:,1].tolist()\n","    x_axis = [datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\") for x in x_axis]\n","    y_axis = y_valid   \n","    lines=plt.plot(x_axis, y_axis, label=\"per-minute real flow - validation\")\n","    plt.setp(lines, color='g', linewidth=1.2, ls='-')\n","    \n","    x_axis = X_test[0:,1].tolist()\n","    x_axis = [datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\") for x in x_axis]\n","    y_axis = y_test   \n","    lines=plt.plot(x_axis, y_axis, label=\"per-minute real flow - validation\")\n","    plt.setp(lines, color='#ffa500', linewidth=1.2, ls='-')\n","     \n","    fig, ax = plt.subplots()   \n","    ax.set_xlabel('Time-interval (Date)')\n","    ax.set_ylabel('Traffic flow (# of Veh/5 min)')\n","    ax.set_title('Traffic flow rate from PeMS real data')\n","    plt.xticks(rotation=70)\n","    # Pad margins so that markers don't get clipped by the axes\n","    plt.margins(.02)\n","    # Tweak spacing to prevent clipping of tick-labels\n","    plt.subplots_adjust(bottom=.1)\n","    \n","    def date2yday(x):\n","        #  x is in matplotlib seconds, so they are numbers.\n","        y = x - mdates.date2num(datetime(2019, 1, 1))\n","        return y\n","    def yday2date(x):\n","        # return a matplotlib datenum (x is seconds since start of year)\n","        y = x + mdates.date2num(datetime(2019, 1, 1))    \n","        return y\n","    \n","    secaxx = ax.secondary_xaxis('top', functions=(date2yday, yday2date))\n","    secaxx.set_xlabel('days from year start [2019]')\n","     \n","    plt.legend(['Per-minute flow - Train dataset', 'Validation', 'Test'], loc='upper left')\n","    fig.set_size_inches(8, 4)\n","    \n","    plt.show()\n","    img = imgUtil.plt2img ( fig )\n","    global_arr_Plots.append([img])\n","    \n","elif RUN_ON_DATASET == 2:\n","    x_axis = X_train[0:,0].tolist()\n","    y_axis = y_train   \n","    \n","    fig, ax = plt.subplots()       \n","    lines=plt.plot(x_axis, y_axis, label=\"Per-minute real flow - train\")\n","    plt.setp(lines, color='b', linewidth=1.2, ls='-')\n","       \n","    x_axis = X_valid[0:,0].tolist()\n","    y_axis = y_valid   \n","    lines=plt.plot(x_axis, y_axis, label=\"per-minute real flow - validation\")\n","    plt.setp(lines, color='g', linewidth=1.2, ls='-')\n","    \n","    x_axis = X_test[0:,0].tolist()\n","    y_axis = y_test   \n","    lines=plt.plot(x_axis, y_axis, label=\"per-minute real flow - validation\")\n","    plt.setp(lines, color='#ffa500', linewidth=1.2, ls='-')\n","    fig.set_size_inches(8, 4)\n","    \n","    plt.show()\n","    img = imgUtil.plt2img ( fig )\n","    global_arr_Plots.append([img])\n","    \n","#############################################\n","### STEP 1: Data Preprocessing for LSTM input\n","### Convert from time series to sliding window \n","#############################################\n","# split a univariate sequence into samples \n","# using sliding window of n_steps\n","def split_sequence_univariate(sequence, n_steps):\n","\tX, y = list(), list()\n","\tfor i in range(len(sequence)):\n","\t\t# find the end of this pattern\n","\t\tend_ix = i + n_steps\n","\t\t# check if we are beyond the sequence\n","\t\tif end_ix > len(sequence)-1:\n","\t\t\tbreak\n","\t\t# gather input and output parts of the pattern\n","\t\tseq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n","\t\tX.append(seq_x)\n","\t\ty.append(seq_y)\n","\treturn array(X), array(y)\n","\n","\n","# define input/training sequence\n","train_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 130, 140, 150]\n","valid_seq = [130, 140, 150, 160, 170, 180, 190, 200]\n","test_seq = [ 180, 190, 200, 210, 220, 230, 240]\n","\n","if RUN_ON_DATASET == 1 or RUN_ON_DATASET == 2:\n","    train_seq = y_train\n","    valid_seq = y_valid\n","    test_seq = y_test\n","elif RUN_ON_DATASET == 0:\n","    times = pd.date_range('2019-10-01', periods=289, freq='5min')\n","    times = np.array(times[0:len(train_seq)])\n","    ds = pd.Series([float(i) for i in train_seq], index=[i for i in times]) # used for ARIMA\n","    \n","# choose a number of time steps in sliding window\n","n_steps = 3\n","# split into samples\n","\n","X_train_seq, y_train_seq = split_sequence_univariate(train_seq, n_steps)\n","X_valid_seq, y_valid_seq = split_sequence_univariate(valid_seq, n_steps)\n","X_test_seq, y_test_seq = split_sequence_univariate(test_seq, n_steps)\n","\n","X_all = np.append(X_train_seq, X_valid_seq, axis=0) \n","X_all = np.append(X_all, X_test_seq, axis=0) \n","y_all = np.append(y_train_seq, y_valid_seq, axis=0)\n","y_all = np.append(y_all, y_test_seq, axis=0) \n","\n","# summarize the data\n","for i in range(min(len(X_all), 5)):\n","\tprint(X_all[i], y_all[i])\n","\n","\n","commulative_results = [];\n","commulative_results.append(['Method Name', 'Epochs', 'RMSE', 'MAPE%', 'MAE', \"Execution Time (sec)\", 'Total', 'Correct', 'Wrong'])\n","\n","def draw_results():        \n","    tab = tt.Texttable()\n","    tab.header(commulative_results[0])\n","    for row in commulative_results[1:]:\n","        tab.add_row(row)\n","    s = tab.draw()\n","    print (s)\n","\n","#############################################\n","### ALGO Set #1: Vanilla LSTM, Stacked LSTM, Bidirectional LSTM, GRU, RNN\n","### Given: X, y with window length = 3\n","# Multiple hidden LSTM layers can be stacked one on top of another \n","# in what is referred to as a Stacked LSTM model. An LSTM layer \n","# requires a three-dimensional input and LSTMs by default will \n","# produce a two-dimensional output as an interpretation from the \n","# end of the sequence.\n","#############################################\n","# univariate lstm example\n","\n","def build_stacked_lstm_model(units, steps, features, act, opt, los):\n","    # define model\n","    model = Sequential()\n","    model.add(LSTM(units, \n","                   activation=act, \n","                   return_sequences=True, \n","                   input_shape=(steps, features)))\n","    model.add(LSTM(units, activation=act))\n","    model.add(Dense(1))\n","   \n","    model.compile(optimizer=opt, loss=los)\n","    model.summary()\n","    return model\n","    \n","def build_vanilla_lstm_model(units, steps, features, act, opt, los):\n","    # define model\n","    model = Sequential()\n","    model.add(LSTM(units, \n","                   activation=act, \n","                   input_shape=(steps, features)))\n","    model.add(Dense(1))\n","    \n","    model.compile(optimizer=optimizer, loss=los)\n","    model.summary()\n","    return model\n","\n","def build_bidirectional_lstm_model(units, steps, features, act, opt, los):\n","\n","# define model\n","    model = Sequential()\n","    model.add(Bidirectional(LSTM(units, activation=act), input_shape=(steps, features)))\n","    model.add(Dense(1))\n","    \n","    model.compile(optimizer=opt, loss=los)\n","    model.summary()\n","    return model\n","\n","def build_gru_model(units, steps, features, act, opt, los):\n","    # define model\n","    model = Sequential()\n","    model.add(GRU(units, \n","                   activation=act, \n","                   return_sequences=True, \n","                   input_shape=(steps, features)))\n","    model.add(GRU(30, return_sequences=True, activation=act))\n","    model.add(GRU(30, activation=act))\n","    model.add(Dense(1))\n","   \n","    model.compile(optimizer=opt, loss=los)\n","    model.summary()\n","    return model\n","\n","def buildMLP(units,  steps, out_dim, act, opt, los):\n","    print('Build MLP...')\n","    model = Sequential()\n","    model.add(Dense(units, activation='relu', input_dim=steps))\n","    model.add(Dense(30, activation='relu'))\n","    model.add(Dense(out_dim, activation='relu'))\n","\n","    model.compile(loss='mean_absolute_error', optimizer='rmsprop')\n","    model.summary()\n","    return model\n","\n","def buildRNN(units,  steps, out_dim, act, opt, los):\n","    print('Build RNN...')\n","    model = Sequential()\n","    model.add(SimpleRNN(out_dim, input_shape=(steps, out_dim)))\n","\n","    model.compile(loss='mean_absolute_error', optimizer='rmsprop')\n","    model.summary()\n","    return model\n","\n"," \n","\n","for a in range(len(ALGOKEYS)):\n","    algo_name = [x[1] for x in ALGOS if x[0] == ALGOKEYS[a]][0]\n","    for j in range(len(EPOCHS)):\n","        start_time = time.time()   \n","        # reshape from [samples, timesteps] into [samples, timesteps, features]\n","        n_features = 1\n","        X = X_train_seq\n","        y = y_train_seq\n","        \n","        X = X.reshape(X.shape[0], X.shape[1], n_features)\n","       \n","        unit_size = 200\n","        n_steps = 3\n","        n_features = 1\n","        activation='relu'\n","        optimizer='adam'\n","        loss='mse'\n","        \n","        validAlgo = True \n","        \n","        if 1001 in ALGOKEYS:\n","            model = build_vanilla_lstm_model(unit_size, n_steps, n_features, activation, optimizer, loss)\n","        elif 1002 in ALGOKEYS:\n","            model = build_stacked_lstm_model(unit_size, n_steps, n_features, activation, optimizer, loss)\n","        elif 1003 in ALGOKEYS:\n","            model = build_bidirectional_lstm_model(unit_size, n_steps, n_features, activation, optimizer, loss)\n","        elif 1006 in ALGOKEYS:\n","            model = build_gru_model(unit_size, n_steps, n_features, activation, optimizer, loss)\n","    #        elif 1013 in ALGOKEYS:\n","    #            model = buildMLP(unit_size, n_steps, n_features, activation, optimizer, loss)\n","        elif 1014 in ALGOKEYS:\n","            model = buildRNN(unit_size, n_steps, n_features, activation, optimizer, loss)        \n","        else:\n","            validAlgo = False        \n","        \n","        if(validAlgo):\n","    #        # fit model\n","            for repeat in range(REPEATS):\n","                #model.fit(X, y, epochs=EPOCHS[j], verbose=0)\n","                #model.fit(train_seq[3:], y, epochs=EPOCHS[j], verbose=0)\n","                model.fit(X, y, epochs=EPOCHS[j], batch_size=64, \n","                      validation_data=(X_valid_seq.reshape(X_valid_seq.shape[0], X_valid_seq.shape[1], n_features), y_valid_seq), \n","                      shuffle=False)\n","                model.reset_states()\n","                \n","            # demonstrate prediction on train + validation + test\n","            arr_yhat = []\n","            for observation in X_all:\n","              #x_input = array([44, 50, 70])\n","              x_input = observation.reshape((1, n_steps, n_features))\n","              #x_input = x_input.reshape((x_input.shape[0], x_input.shape[1], n_features))\n","              yhat = model.predict(x_input, verbose=0)\n","              arr_yhat.append(yhat[0][0])\n","            \n","            arr_y = y_all\n","            arr_x = [i for i in range(len(X_all))]\n","            \n","            fig, ax = plt.subplots()\n","            ax.set_title('Method: %s with Epochs: %d' % (algo_name, EPOCHS[j]))\n","            ax.set_xlabel('Observation count')\n","            ax.set_ylabel('Traffic flow (# of Veh/5 min)')\n","            plt.plot(arr_x, arr_y, 'r', label='Actual traffic flow',linewidth=1.5)\n","            plt.plot(arr_x, arr_yhat, '-g', label=algo_name, linewidth=1.2)\n","            plt.legend()            \n","            fig.set_size_inches(8, 4)\n","    \n","            plt.show()\n","            img = imgUtil.plt2img ( fig )\n","            global_arr_Plots.append([img])\n","            \n","            \n","#            global_arr_Plots.append([algo_name, EPOCHS[j], ax.canvas.draw()])\n","          \n","            print(\"===============Data Set=======================\")\n","            print(\"Observation, Label, Prediction\")\n","            # summarize the data\n","            for i in range(min(len(X_all), 15)):\n","            \tprint([a for a in X_all[i]], arr_y[i], arr_yhat[i] )\n","               \n","            \n","            print(\"===============RMSE=================\")\n","            # report RMSE performance\n","            rmse = sqrt(mean_squared_error(arr_y, arr_yhat))\n","            mae = mean_absolute_error(arr_y, arr_yhat)\n","            mape = mean_absolute_percentage_error(arr_y, arr_yhat)\n","            print('RMSE: %.3f, MAPE: %.1f, MAE: %.3f, EPOCHS: %d, Algo: %s' % (rmse, mape, mae, EPOCHS[j], algo_name))\n","            \n","        \n","            # correctness\n","            total = 0\n","            correct = 0\n","            wrong = 0\n","            threshold = 5.0\n","            for i in range(len(arr_yhat)):\n","              total=total+1\n","              \n","              if((arr_y[i] - arr_yhat[i]) < threshold):\n","                correct=correct+1\n","              else:\n","                wrong=wrong+1\n","            \n","            end_time = time.time()\n","            \n","            \n","            commulative_results.append([algo_name, EPOCHS[j], round(rmse, 2),round(mape, 2), round(mae, 2), round(end_time - start_time, 2), total, correct, wrong])\n","            \n","        draw_results()  \n","        print(\"===============End of \"+algo_name+\"=======================\")\n","\n","#############################################\n","### ALGO #1: DBN\n","### Given: X, y with window length = 3\n","# **Vanilla LSTM**\n","#A Vanilla LSTM is an LSTM model that has a single hidden layer of \n","#LSTM units, and an output layer used to make a prediction.\n","#############################################\n","if 1011 in ALGOKEYS:   #DBN\n","    algo_name = [x[1] for x in ALGOS if x[0] == 1011][0]\n","    for j in range(len(EPOCHS)):\n","        start_time = time.time()   \n","        # reshape from [samples, timesteps] into [samples, timesteps, features]\n","        n_features = 1\n","        X = X_train_seq\n","        y = y_train_seq\n","        \n","        # Data scaling\n","        min_max_scaler = MinMaxScaler()\n","        X = min_max_scaler.fit_transform(X)\n","        \n","        # Training\n","        regressor = SupervisedDBNRegression(hidden_layers_structure=[100],\n","                                            learning_rate_rbm=0.01,\n","                                            learning_rate=0.01,\n","                                            n_epochs_rbm=EPOCHS[j], #20,\n","                                            n_iter_backprop=200,\n","                                            batch_size=16,\n","                                            activation_function='relu')\n","        regressor.fit(X, y)\n","        \n","        # Test\n","        X_test = min_max_scaler.transform(X_all)\n","        arr_yhat = regressor.predict(X_test)\n","     \n","        arr_y = y_all\n","        arr_x = [i for i in range(len(X_all))]\n","        arr_yhat = [round(i[0], 3) for i in arr_yhat]\n","        \n","        fig, ax = plt.subplots()\n","        ax.set_title('Method: %s with Epochs: %d' % (algo_name, EPOCHS[j]))\n","        ax.set_xlabel('Observation count')\n","        ax.set_ylabel('Traffic flow (# of Veh/5 min)')\n","        plt.plot(arr_x, arr_y, 'r', label='Actual traffic flow',linewidth=1.5)\n","        plt.plot(arr_x, arr_yhat, '-g', label=algo_name, linewidth=1.2)\n","    #    plt.rcParams[\"figure.figsize\"] = (16,6)\n","        plt.legend()\n","        fig.set_size_inches(8, 4)\n","    \n","        plt.show()\n","        img = imgUtil.plt2img ( fig )\n","        global_arr_Plots.append([img])\n","                \n","        print(\"===============Data Set=======================\")\n","        print(\"Observation, Label, Prediction\")\n","        # summarize the data\n","        for i in range(min(len(X_all), 15)):\n","        \tprint([a for a in X_all[i]], arr_y[i], arr_yhat[i])\n","           \n","        \n","        print(\"===============RMSE=================\")\n","        # report RMSE performance\n","        rmse = sqrt(mean_squared_error(arr_y, arr_yhat))\n","        mae = mean_absolute_error(arr_y, arr_yhat)\n","        mape = mean_absolute_percentage_error(arr_y, arr_yhat)\n","        print('RMSE: %.3f, MAPE: %.1f, MAE: %.3f, EPOCHS: %d, Algo: %s' % (rmse, mape, mae, EPOCHS[j], algo_name))\n","        \n","    \n","        # correctness\n","        total = 0\n","        correct = 0\n","        wrong = 0\n","        threshold = 5.0\n","        for i in range(len(arr_yhat)):\n","          total=total+1\n","          \n","          if((arr_y[i] - arr_yhat[i]) < threshold):\n","            correct=correct+1\n","          else:\n","            wrong=wrong+1\n","        \n","        end_time = time.time()\n","        \n","        \n","        commulative_results.append([algo_name, EPOCHS[j], round(rmse, 2), round(mape, 2), round(mae, 2), round(end_time - start_time, 2), total, correct, wrong])\n","    \n","    draw_results()  \n","    print(\"===============End of \"+algo_name+\"=======================\")\n","\n","#############################################\n","### ALGO #222: MLP\n","### Given: X, y with window length = 3\n","# **Vanilla LSTM**\n","#A Vanilla LSTM is an LSTM model that has a single hidden layer of \n","#LSTM units, and an output layer used to make a prediction.\n","#############################################\n","if 1013 in ALGOKEYS:   # MLP\n","    algo_name = [x[1] for x in ALGOS if x[0] == 1013][0]\n","    for j in range(len(EPOCHS)):\n","        start_time = time.time()   \n","        # reshape from [samples, timesteps] into [samples, timesteps, features]\n","        n_features = 1\n","        X = X_train_seq\n","        y = y_train_seq\n","        \n","        X = X.reshape(X.shape[0], X.shape[1], n_features)\n","       \n","        unit_size = 200\n","        n_steps = 3\n","        n_features = 1\n","        activation='relu'\n","        optimizer='adam'\n","        loss='mse'\n","    \n","        model = buildMLP(unit_size, X.shape[0], n_features, activation, optimizer, loss)\n","    \n","        model.fit(X, y, epochs=EPOCHS[j], verbose=0)\n","    \n","    #        # fit model\n","    #        for repeat in range(REPEATS):\n","    #            model.fit(X, y, epochs=EPOCHS[j], verbose=0)\n","    #            model.fit(X, y, epochs=EPOCHS[j], batch_size=64, \n","    #                  validation_data=(X_valid_seq.reshape(X_valid_seq.shape[0], X_valid_seq.shape[1], n_features), y_valid_seq), \n","    #                  shuffle=False)\n","    #            model.reset_states()\n","    #           \n","        #model.fit(X, y, epochs=EPOCHS[j], verbose=0)\n","    #        model.fit(X, y, epochs=EPOCHS[j], batch_size=64, \n","    #                  validation_data=(X_valid_seq.reshape(X_valid_seq.shape[0], X_valid_seq.shape[1], n_features), y_valid_seq), \n","    #                  shuffle=False)\n","             \n","            \n","        # demonstrate prediction on train + validation + test\n","        arr_yhat = []\n","        for observation in X_all:\n","          #x_input = array([44, 50, 70])\n","          x_input = observation.reshape((1, n_steps, n_features))\n","          #x_input = x_input.reshape((x_input.shape[0], x_input.shape[1], n_features))\n","          yhat = model.predict(x_input, verbose=0)\n","          arr_yhat.append(yhat[0][0])\n","        \n","        arr_y = y_all\n","        arr_x = [i for i in range(len(X_all))]\n","        \n","        fig, ax = plt.subplots()\n","        ax.set_title('Method: %s with Epochs: %d' % (algo_name, EPOCHS[j]))\n","        ax.set_xlabel('Observation count')\n","        ax.set_ylabel('Traffic flow (# of Veh/5 min)')\n","        plt.plot(arr_x, arr_y, 'r', label='Actual traffic flow',linewidth=1.5)\n","        plt.plot(arr_x, arr_yhat, '-g', label=algo_name, linewidth=1.2)\n","    #    plt.rcParams[\"figure.figsize\"] = (16,6)\n","        plt.legend()\n","        fig.set_size_inches(8, 4)\n","    \n","        plt.show()\n","        img = imgUtil.plt2img ( fig )\n","        global_arr_Plots.append([img])\n","        \n","        print(\"===============Data Set=======================\")\n","        print(\"Observation, Label, Prediction\")\n","        # summarize the data\n","        for i in range(min(len(X_all), 15)):\n","        \tprint([a for a in X_all[i]], arr_y[i], arr_yhat[i] )\n","           \n","        \n","        print(\"===============RMSE=================\")\n","        # report RMSE performance\n","        rmse = sqrt(mean_squared_error(arr_y, arr_yhat))\n","        mae = mean_absolute_error(arr_y, arr_yhat)\n","        mape = mean_absolute_percentage_error(arr_y, arr_yhat)\n","        print('RMSE: %.3f, MAPE: %.1f, MAE: %.3f, EPOCHS: %d, Algo: %s' % (rmse, mape, mae, EPOCHS[j], algo_name))\n","        \n","    \n","        # correctness\n","        total = 0\n","        correct = 0\n","        wrong = 0\n","        threshold = 5.0\n","        for i in range(len(arr_yhat)):\n","          total=total+1\n","          \n","          if((arr_y[i] - arr_yhat[i]) < threshold):\n","            correct=correct+1\n","          else:\n","            wrong=wrong+1\n","        \n","        end_time = time.time()\n","        \n","        \n","        commulative_results.append([algo_name, EPOCHS[j], round(rmse, 2),round(mape, 2), round(mae, 2), round(end_time - start_time, 2), total, correct, wrong])\n","        \n","    draw_results()  \n","    print(\"===============End of \"+algo_name+\"=======================\")\n","\n","#############################################\n","### ALGO #222: ARIMA\n","### Given: X, y with window length = 3\n","# **ARIMA**\n","#A Vanilla LSTM is an LSTM model that has a single hidden layer of \n","#LSTM units, and an output layer used to make a prediction.\n","#############################################\n","if 1007 in ALGOKEYS:\n","    algo_name = [x[1] for x in ALGOS if x[0] == 1007][0]\n","    \n","    start_time = time.time()   \n","    \n","    # fit model\n","    model = ARIMA(ds, order=(5,1,0))\n","    model_fit = model.fit(disp=0)\n","    print(model_fit.summary())\n","    # plot residual errors\n","    residuals = DataFrame(model_fit.resid)\n","    residuals.plot()\n","    plt.show()\n","    residuals.plot(kind='kde')\n","    plt.show()\n","    print(residuals.describe())\n","    \n","    ##################ROLLING FORECAST ARIMA#########\n","    X = ds.values\n","    size = int(len(X) * 0.66)\n","    train, test = X[0:size], X[size:len(X)]\n","    history = [x for x in train]\n","    predictions = list()\n","    for t in range(len(test)):\n","    \tmodel = ARIMA(history, order=(5,1,0))\n","    \tmodel_fit = model.fit(disp=0)\n","    \toutput = model_fit.forecast()\n","    \tyhat = output[0]\n","    \tpredictions.append(yhat)\n","    \tobs = test[t]\n","    \thistory.append(obs)\n","    \tprint('predicted=%.3f, expected=%.3f' % (yhat, obs))\n","        \n","    error = mean_squared_error(test, predictions)\n","    print('Test MSE: %.3f' % error)\n","   ###################\n","    arr_y = test\n","    arr_yhat = predictions\n","    rmse = sqrt(mean_squared_error(arr_y, arr_yhat))\n","    mae = mean_absolute_error(arr_y, arr_yhat)\n","    mape = mean_absolute_percentage_error(arr_y, arr_yhat)\n","    print('RMSE: %.3f, MAPE: %.1f, MAE: %.3f, EPOCHS: %d, Algo: %s' % (rmse, mape, mae, EPOCHS[j], algo_name))\n","    ##################\n","    # plot\n","    fig, ax = plt.subplots()\n","    ax.set_title('Method: %s with Epochs: %d' % (algo_name, EPOCHS[j]))\n","    ax.set_xlabel('Observation count')\n","    ax.set_ylabel('Traffic flow (# of Veh/5 min)')\n","    plt.plot(test)\n","    plt.plot(predictions, color='red')\n","    fig.set_size_inches(8, 4)\n","\n","    plt.show()\n","    img = imgUtil.plt2img ( fig )\n","    global_arr_Plots.append([img])\n","     \n","    ##################ROLLING FORECAST ARIMA#########\n","    # https://towardsdatascience.com/machine-learning-part-19-time-series-and-autoregressive-integrated-moving-average-model-arima-c1005347b0d7\n","    rolling_mean = ds.rolling(window = 6).mean()\n","    rolling_std = ds.rolling(window = 6).std()\n","    plt.plot(ds, color = 'blue', label = 'Original')\n","    plt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')\n","    plt.plot(rolling_std, color = 'black', label = 'Rolling Std')\n","    plt.legend(loc = 'best')\n","    plt.title('Rolling Mean & Rolling Standard Deviation')\n","    plt.show()\n","    \n","    end_time = time.time()\n","    commulative_results.append([algo_name, 'N/A', round(rmse, 2),round(mape, 2), round(mae, 2), round(end_time - start_time, 2), -1, -1, -1])\n","    draw_results()  \n","    print(\"===============End of \"+algo_name+\"=======================\")\n"," \n","\n","   \n","    \n","    \n","    \n","#############################################\n","### ALGO #4: CNN LSTM\n","### Given: X, y with window length = 3\n","# **CNN LSTM**\n","# A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.\n","# The CNN can be very effective at automatically extracting and learning features from one-dimensional sequence data such as univariate time series data.\n","# A CNN model can be used in a hybrid model with an LSTM backend where the CNN is used to interpret subsequences of input that together are provided as a sequence to an LSTM model to interpret. This hybrid model is called a CNN-LSTM.\n","#############################################\n","def build_cnn_lstm_model():\n","    model = Sequential()\n","    model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))\n","    model.add(TimeDistributed(MaxPooling1D(pool_size=2)))\n","    model.add(TimeDistributed(Flatten()))\n","    model.add(LSTM(50, activation='relu'))\n","    model.add(Dense(1))\n","    model.compile(optimizer='adam', loss='mse')\n","    model.summary()\n","    return model\n","\n","def build_convLstm_model():\n","    model = Sequential()\n","    model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n","    model.add(Flatten())\n","    model.add(Dense(1))\n","    model.compile(optimizer='adam', loss='mse')\n","    model.summary()    \n","    return model\n","\n","# univariate lstm example\n","if 2001 in ALGOKEYS or 2002 in ALGOKEYS:\n","    for a in range(len(ALGOKEYS)):\n","        algo_name = [x[1] for x in ALGOS if x[0] == ALGOKEYS[a]][0]\n","\n","    #############################################\n","    ## Data pre-processing startes here\n","    # choose a number of time steps in sliding window\n","    n_steps = 4\n","    # split into samples\n","    \n","    X_train_seq, y_train_seq = split_sequence_univariate(train_seq, n_steps)\n","    X_valid_seq, y_valid_seq = split_sequence_univariate(valid_seq, n_steps)\n","    X_test_seq, y_test_seq = split_sequence_univariate(test_seq, n_steps)\n","    \n","    X_all = np.append(X_train_seq, X_valid_seq, axis=0) \n","    X_all = np.append(X_all, X_test_seq, axis=0) \n","    y_all = np.append(y_train_seq, y_valid_seq, axis=0)\n","    y_all = np.append(y_all, y_test_seq, axis=0) \n","    \n","    # summarize the data\n","    for i in range(min(len(X_all), 5)):\n","    \tprint(X_all[i], y_all[i])\n","\n","    X = X_train_seq\n","    y = y_train_seq\n","    \n","    n_features = 1\n","    n_seq = 2\n","    n_steps = 2\n","    X = X.reshape(X.shape[0], n_seq, n_steps, n_features)\n","     \n","    # summarize the data\n","    for i in range(min(len(X), 5)):\n","    \tprint(X[i], y[i])\n","    # ---------------------------------|\n","    #   sample|  label   |  prediction |\n","    # ---------------------------------|\n","    #   X     |  y       |  yhat       | << given data\n","    #   Xunk  |  yunk    |  yunkhat    | << unknown samples \n","    # ---------------------------------|\n","    #   Xall  |  yall    |  yallhat    | << unknown samples \n","    # ---------------------------------|\n","    \n","    \n","    #############################################\n","    ## Data pre-processing ends here\n","    #############################################\n","    \n","    for j in range(len(arr_Epochs)):\n","        start_time = time.time() \n","        \n","       \n","        # define model\n","        if 2001 in ALGOKEYS:\n","            model = build_cnn_lstm_model()\n","        elif 2002 in ALGOKEYS:\n","            model = build_convLstm_model()\n","\n","        # fit model\n","        model.fit(X, y, epochs=arr_Epochs[j], verbose=0)\n","        # demonstrate prediction\n","        arr_yhat = []\n","        for observation in X_all:\n","          #x_input = array([44, 50, 70])\n","          #x_input = observation.reshape((1, n_steps, n_features))0\n","          x_input = observation.reshape((1, n_seq, n_steps, n_features))\n","          #x_input = x_input.reshape((x_input.shape[0], x_input.shape[1], n_features))\n","          yhat = model.predict(x_input, verbose=0)\n","          arr_yhat.append(yhat[0][0])\n","        \n","        arr_y = y_all\n","        arr_x = [i for i in range(len(X_all))]\n","        \n","        fig, ax = plt.subplots()\n","        ax.set_title('Method: %s with Epochs: %d' % (algo_name, EPOCHS[j]))\n","        ax.set_xlabel('Observation count')\n","        ax.set_ylabel('Traffic flow (# of Veh/5 min)')\n","        plt.plot(arr_x, arr_y, 'r', label='Actual traffic flow',linewidth=2.0)\n","        plt.plot(arr_x, arr_yhat, '-g', label=algo_name, linewidth=2.0)\n","    #    plt.rcParams[\"figure.figsize\"] = (16,6)\n","        plt.legend()\n","        fig.set_size_inches(8, 4)\n","    \n","        plt.show()\n","        img = imgUtil.plt2img ( fig )\n","        global_arr_Plots.append([img])\n","\n","        \n","        \n","        print(\"===============Data Set=======================\")\n","        print(\"Observation, Label, Prediction\")\n","        # summarize the data\n","        for i in range(min(len(X_all), 15)):\n","        \tprint([a for a in X_all[i]], arr_y[i], arr_yhat[i] )\n","           \n","        \n","        print(\"===============RMSE=================\")\n","        # report RMSE performance\n","        rmse = sqrt(mean_squared_error(arr_y, arr_yhat))\n","        mae = mean_absolute_error(arr_y, arr_yhat)\n","        mape = mean_absolute_percentage_error(arr_y, arr_yhat)\n","        print('RMSE: %.3f, MAPE: %.1f, MAE: %.3f, EPOCHS: %d, Algo: %s' % (rmse, mape, mae, EPOCHS[j], algo_name))\n","       \n","    \n","        # correctness\n","        total = 0\n","        correct = 0\n","        wrong = 0\n","        threshold = 2.0\n","        for i in range(len(arr_yhat)):\n","          total=total+1\n","          \n","          if((arr_y[i] - arr_yhat[i]) < threshold):\n","            correct=correct+1\n","          else:\n","            wrong=wrong+1\n","        \n","        end_time = time.time()\n","        \n","        \n","        commulative_results.append([algo_name, EPOCHS[j], round(rmse, 2),round(mape, 2), round(mae, 2), round(end_time - start_time, 2), total, correct, wrong])\n","    \n","    \n","    print(\"===============overall results=================\")\n","   \n","    draw_results()  \n","    print(\"===============End of \"+algo_name+\"=======================\")\n","\n","#\n","##############################################\n","#### ALGO #5: ConvLSTM\n","#### Given: X, y with window length = 3\n","## **ConvLSTM**\n","##A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the \n","##convolutional reading of input is built directly into each LSTM \n","##unit. The ConvLSTM was developed for reading two-dimensional \n","##spatial-temporal data, but can be adapted for use with univariate \n","##time series forecasting.\n","##############################################\n","#from keras.layers import ConvLSTM2D\n","# \n","##############################################\n","## univariate lstm example\n","#algo_name = 'ConvLSTM'\n","#\n","##############################################\n","### Data pre-processing startes here\n","## choose a number of time steps in sliding window\n","#n_steps = 4\n","## split into samples\n","#X, y = split_sequence(raw_seq, n_steps)\n","## reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]\n","#n_features = 1\n","#n_seq = 2\n","#n_steps = 2\n","#X = X.reshape(X.shape[0], n_seq, 1, n_steps, n_features)\n","# \n","## summarize the data\n","#for i in range(min(len(X), 5)):\n","#\tprint(X[i], y[i])\n","## ---------------------------------|\n","##   sample|  label   |  prediction |\n","## ---------------------------------|\n","##   X     |  y       |  yhat       | << given data\n","##   Xunk  |  yunk    |  yunkhat    | << unknown samples \n","## ---------------------------------|\n","##   Xall  |  yall    |  yallhat    | << unknown samples \n","## ---------------------------------|\n","#\n","#\n","##unknown_seq = [70, 80, 90, 100, 110, 120]\n","## split into samples\n","#n_steps = 4\n","#Xunk, yunk = split_sequence(unknown_seq, n_steps)\n","#n_steps = 2\n","#Xunk = Xunk.reshape(Xunk.shape[0], n_seq, 1, n_steps, n_features)\n","#\n","##Xunk = [[70, 80, 90], [80, 90, 100]]\n","##yunk = [100, 110]\n","#\n","#Xall = np.append(X, Xunk, axis=0) \n","#yall = np.append(y, yunk, axis=0)\n","##############################################\n","### Data pre-processing ends here\n","##############################################\n","#\n","#for j in range(len(arr_Epochs)):\n","#    start_time = time.time() \n","#\n","#    # define model\n","#    model = Sequential()\n","#    model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n","#    model.add(Flatten())\n","#    model.add(Dense(1))\n","#    model.compile(optimizer='adam', loss='mse')\n","#    model.summary()\n","#    # fit model\n","#    model.fit(X, y, epochs=arr_Epochs[j], verbose=0)\n","#    # demonstrate prediction\n","#    arr_yhat = []\n","#    for observation in Xall:\n","#      #x_input = array([44, 50, 70])\n","#      #x_input = observation.reshape((1, n_steps, n_features))0\n","#      x_input = observation.reshape((1, n_seq, 1, n_steps, n_features))\n","#      #x_input = x_input.reshape((x_input.shape[0], x_input.shape[1], n_features))\n","#      yhat = model.predict(x_input, verbose=0)\n","#      arr_yhat.append(yhat[0][0])\n","#    \n","#    arr_y = yall\n","#    arr_x = [i for i in range(len(Xall))]\n","#    \n","#    ax = plt.subplots()\n","#    plt.plot(arr_x, arr_y, 'r', label='Actual traffic flow',linewidth=2.0)\n","#    plt.plot(arr_x, arr_yhat, '-g', label=algo_name, linewidth=2.0)\n","##    plt.rcParams[\"figure.figsize\"] = (16,6)\n","#    plt.legend()\n","#    plt.show()\n","#    \n","#    \n","#    print(\"===============Data Set=======================\")\n","#    print(\"Observation, Label, Prediction\")\n","#    # summarize the data\n","#    for i in range(min(len(Xall), 5)):\n","#    \tprint([a for a in Xall[i]], arr_y[i], arr_yhat[i] )\n","#       \n","#    \n","#    print(\"===============RMSE=================\")\n","#    # report RMSE performance\n","#    rmse = sqrt(mean_squared_error(arr_y, arr_yhat))\n","#    mae = mean_absolute_error(arr_y, arr_yhat)\n","#    mape = mean_absolute_percentage_error(arr_y, arr_yhat)\n","#    print('RMSE: %.3f, MAPE: %.1f, MAE: %.3f, EPOCHS: %d, Algo: %s' % (rmse, mape, mae, EPOCHS[j], algo_name))\n","#  \n","#\n","#    # correctness\n","#    total = 0\n","#    correct = 0\n","#    wrong = 0\n","#    threshold = 2.0\n","#    for i in range(len(arr_yhat)):\n","#      total=total+1\n","#      \n","#      if((arr_y[i] - arr_yhat[i]) < threshold):\n","#        correct=correct+1\n","#      else:\n","#        wrong=wrong+1\n","#    \n","#    end_time = time.time()\n","#    \n","#    \n","#    commulative_results.append([algo_name, arr_Epochs[j], round(rmse, 2), round(mae, 2), round(end_time - start_time, 2), total, correct, wrong])\n","#\n","#\n","#print(\"===============overall results=================\")\n","#for i in range(len(commulative_results)):\n","#\tprint(commulative_results[i])\n","##    \n","##print(\"===============boxplot analysis=================\")\n","##error_scores = [i[2] for i in  commulative_results[1:]]\n","### summarize results\n","##results = dataframe()\n","##results['rmse'] = error_scores\n","##print(results.describe())\n","##results.boxplot()\n","###plt.gcf().set_size_inches(18, 6)\n","##plt.show()\n","#\n","#\n","##print(\"===============Learning rate Epochs vs RMSE=======================\")\n","##ax = plt.subplots()\n","##plt.plot(arr_Epochs, error_scores, 'b', label='Epochs vs RMSE',linewidth=2.0)\n","##plt.legend()\n","##plt.show()\n","#\n","#print(\"===============End of LSTM=======================\")\n","#print(\"===============End of LSTM=======================\")\n","#print(\"===============End of LSTM=======================\")\n","#\n","\n","#############################################\n","## OVERALL POST-BOX PLOT ANALYSIS Starts here\n","#############################################\n","\n","##POST PROCESSING\n","algo_list = [i[0] for i in  commulative_results[1:]]\n","# get distinct values\n","algo_set = set(algo_list)\n","# from set to list\n","algo_set = list(algo_set)\n","\n","box_data_desc = []\n","box_data_dict = {}\n","for i in range(len(algo_set)):\n","    algo_result = [x for x  in commulative_results if x[0] == algo_set[i]]\n","    print(\"===============# \" + str(i+1) + \": boxplot analysis of \"+algo_set[i]+\"=================\")\n","    error_scores_rmse = [i[2] for i in  algo_result]\n","    \n","    box_data_dict[algo_set[i]] = error_scores_rmse\n","    \n","    results = DataFrame()\n","    results['rmse'] = error_scores_rmse\n","    print(results.describe())\n","    box_data_desc.append(results.describe())\n","\n","\n","fig, ax = plt.subplots()\n","bp = ax.boxplot(box_data_dict.values(), patch_artist=True)\n","ax.set_xticklabels(box_data_dict.keys())\n","ax.set_xlabel('NN Algorithm')\n","ax.set_ylabel('RMSE values')\n","ax.set_title('BoxPlot')\n","plt.xticks(rotation=70)\n","\n","box_colors = ['#5975A4', '#5F9E6E', '#B55D60', '#857AAA', '#ED008C']\n","k = 0\n","## change outline color, fill color and linewidth of the boxes\n","for box in bp['boxes']:\n","    # change outline color\n","    box.set( color='#7570b3', linewidth=2)\n","    # change fill color\n","    box.set( facecolor = box_colors[k] )\n","    k = k + 1\n","\n","## change color and linewidth of the whiskers\n","for whisker in bp['whiskers']:\n","    whisker.set(color='#7570b3', linewidth=2)\n","\n","## change color and linewidth of the caps\n","for cap in bp['caps']:\n","    cap.set(color='#7570b3', linewidth=2)\n","\n","## change color and linewidth of the medians\n","for median in bp['medians']:\n","    median.set(color='#b2df8a', linewidth=2)\n","\n","## change the style of fliers and their fill\n","for flier in bp['fliers']:\n","    flier.set(marker='o', color='#e7298a', alpha=0.5)\n","              \n","#     # summarize results\n","#     results = DataFrame()\n","#     results['rmse'] = error_scores\n","#     print(results.describe())\n","#     results.boxplot()\n","#     #plt.gcf().set_size_inches(18, 6)\n","    \n","plt.gcf().set_size_inches(18, 6)   \n","plt.show()\n","\n","print(\"===============overall results=================\")\n","for i in range(len(commulative_results)):\n","\tprint(commulative_results[i]) \n","    \n"," \n","for i, d in enumerate(commulative_results):\n","    line = '|'.join(str(x).ljust(12) for x in d)\n","    print(line)\n","    if i == 0:\n","        print('-' * len(line))\n","\n","\n","\n","for k in range(len(global_arr_Plots)):\n","    img = global_arr_Plots[k][0] \n","    display(img)\n"],"execution_count":5,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-26e753dac752>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimageUtil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimgUtil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSupervisedDBNRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'imageUtil'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"ECL-pSJhk8RH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":168},"outputId":"dfac629c-6032-4456-e4f3-ce04b654b740","executionInfo":{"status":"ok","timestamp":1575574870066,"user_tz":-300,"elapsed":8098,"user":{"displayName":"Basharat Hussain","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDXtH5g0C7HMyOERBmIREHU1ZvtrNx98-DPcRb_=s64","userId":"13397079541735527718"}}},"source":["pip install texttable\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting texttable\n","  Downloading https://files.pythonhosted.org/packages/82/a8/60df592e3a100a1f83928795aca210414d72cebdc6e4e0c95a6d8ac632fe/texttable-1.6.2.tar.gz\n","Building wheels for collected packages: texttable\n","  Building wheel for texttable (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for texttable: filename=texttable-1.6.2-cp36-none-any.whl size=10654 sha256=a2183173873198d1aebba6335e6c68c7bd7209ab87f2fcfe43a53911af3e9f47\n","  Stored in directory: /root/.cache/pip/wheels/51/d1/d6/dfbe4eb3c468832f7fbe4bd27f9875fa97277cabed8fb6715c\n","Successfully built texttable\n","Installing collected packages: texttable\n","Successfully installed texttable-1.6.2\n"],"name":"stdout"}]}]}